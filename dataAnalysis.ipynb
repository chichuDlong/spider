{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Data Analysis\n",
    "- 正则  \n",
    "- bs4  \n",
    "- xpath\n",
    "---\n",
    "#### 常用正则表达式\n",
    "> 单字符：\n",
    "- . ：处换行外所有字符  \n",
    "- [] ：[aoe] [a-w] 匹配集合中任意一个字符  \n",
    "- \\d ：数字 [0-9]  \n",
    "- \\D ：非数字  \n",
    "- \\w ：数字、字母、下划线、中文  \n",
    "- \\W ：非\\w  \n",
    "- \\s ：所有非空白字符，等价于[\\f\\n\\r\\t\\v]  \n",
    "- \\S ：非空白  \n",
    "> 数量修饰：\n",
    "- \\* ：任意多次，>=0  \n",
    "- \\+ ：至少1次 >=1  \n",
    "- ? ：可有可无，0次或者1次  \n",
    "- {m} ：固定m次 hello{3,}  \n",
    "- {m,} ：至少m次  \n",
    "- {m,n} ：至少m-n次  \n",
    "> 边界：\n",
    "- $ ：以某某结尾\n",
    "- ^ ：以某某开头\n",
    "> 分组：\n",
    "- (ab)\n",
    "- 贪婪模式：.*\n",
    "- 非贪婪模式(惰性)模式：.*?\n",
    "- re.I ：忽略大小写\n",
    "- re.M ：多行匹配\n",
    "- re.S ：单行匹配\n",
    "- re.sub(正则表达式，替换内容，字符串)\n",
    "---\n",
    "#### Mission 1：常用正则练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "hello world\n",
      "['170']\n",
      "['http://', 'https://']\n",
      "hello\n",
      "hit.\n",
      "['saas', 'sas']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 提取出Python\n",
    "key='javapythonc++php'\n",
    "res=re.findall('python',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出hello word\n",
    "key='<html><h1>hello world<h1></html>'\n",
    "res=re.findall('<h1>(.*)<h1>',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出170\n",
    "string='我身高是170'\n",
    "res=re.findall('\\d+',string)\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出http://和https://\n",
    "key='http://www.baidu.com and https://boob.com'\n",
    "res=re.findall('https?://',key)\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出hello\n",
    "key='lalala<html>hello</html>hahah'\n",
    "res=re.findall('<[Hh][Tt][mM][lL]>(.*)</[Hh][Tt][mM][lL]>',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出hit.\n",
    "key='bobo@hit.edu.com'\n",
    "res=re.findall('h.*?\\.',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出sas和saas\n",
    "key='saas and sas and saaaas'\n",
    "res=re.findall('sa{1,2}s',key)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 2：爬取图片数据\n",
    "- text——字符串型数据  \n",
    "- content——2进制型数据  \n",
    "- json——对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "if __name__ == \"__main__\":\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://tse1-mm.cn.bing.net/th/id/R-C.47d5fd52caf4fce40d910c06a7282572?rik=OKBN%2btbwWi3Gxw&riu=http%3a%2f%2fwww.sa.buaa.edu.cn%2f__local%2f4%2f7D%2f5F%2fD52CAF4FCE40D910C06A7282572_411891C6_7668.jpg&ehk=O84%2fcaojbXEwILEhWQwc9ZCtqddou1P4SxY3upbS4g0%3d&risl=&pid=ImgRaw&r=0'\n",
    "    img_data = requests.get(url=url,headers=headers).content\n",
    "    with open('hallThruster.jpg','wb') as fp:\n",
    "        fp.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 3：批量获取图片数据——正则方法\n",
    "- step 1：通过爬虫获取整个页面  \n",
    "- step 2：正则解析数据获取需要信息所在地址  \n",
    "- step 3：根据所需信息地址发起请求获取响应数据  \n",
    "- step 4：持久化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片下载完成！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    #UA伪装\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    if not os.path.exists('./picture_libs'):#如果路径不存在，则用mkdir创建一个单级目录\n",
    "        os.mkdir('./picture_libs')        \n",
    "    #这个最大的for循环是用来执行翻页操作的，由于页数不知道，我这边是随便指定结束页尾的\n",
    "    for pageNum in range(1, 6):\n",
    "        # 当你看到输出语句存在 %d， %s， %f等百分号的时候，就接触到python字符串格式化输出相关知识。\n",
    "        url = 'https://desk.zol.com.cn/fengjing/%d.html'#1.指定url\n",
    "        new_url = format(url%pageNum)#学习format函数的用法\n",
    "        #2.发起请求    3.获取响应数据\n",
    "        page_text = requests.get(url=new_url, headers=headers).text\n",
    "\n",
    "        #存储以及打印出page_text中的内容\n",
    "        # with open('./content','w',encoding='utf-8') as fp:\n",
    "        #     fp.write(page_text)\n",
    "        #     fp.write('\\n')#换行\n",
    "\n",
    "        #正则数据解析\n",
    "        ex = 'class=\"photo-list-.*?src=\"(.*?)\"\\stitle\\s=\\s\".*?\".*?<ins>.*?</ins>'\n",
    "        # 根据正则表达式ex提取page_text中的内容，re.S为单行匹配\n",
    "        img_src_list = re.findall(ex, page_text, re.S)\n",
    "        # print(img_src_list)\n",
    "        i = 0\n",
    "        for src in img_src_list:#遍历img_src_list，即1.指定url\n",
    "            #2.发起请求 3.获取响应数据，注意这里的是图片数据，用的是content\n",
    "            img_data = requests.get(url=src, headers=headers).content#对每一个\n",
    "            # print(img_data)\n",
    "            # 将图片地址以字符‘/’分割，最后一个字符串为图片原来地址，为方便区分人为指定图片名称\n",
    "            # img_name=src.split('/')[-1] \n",
    "            i = i + 1\n",
    "            str2 = str(pageNum)+'_'+str(i)#图片的定位是页数+在这页的位置\n",
    "            img_name = str2+'.jpg'#图片名称\n",
    "            img_path = './picture_libs/'+img_name#图片路径\n",
    "            with open(img_path, 'wb', )as fp:#4.持久化存储\n",
    "                fp.write(img_data)\n",
    "                time.sleep(0.5)\n",
    "            # print(img_name,'下载成功！')\n",
    "    print('图片下载完成！')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 4：bs4数据解析(仅限Python)，环境：```pip install bs4```，```pip install lxml```\n",
    "> 数据解析原理：\n",
    ">> 1、标签定位；<br>\n",
    ">> 2、提取标签、标签属性中存储的数据值；\n",
    "\n",
    "> bs4数据解析原理：\n",
    ">> 1、实例化一个BeautifulSoup对象，并且将页面源码数据加载至该对象中；<br>\n",
    ">> 2、通过调用BeautifulSoup对象中相关属性或方法进行标签定位和数据提取；\n",
    "\n",
    "> 如何实例化一个BeautifulSoup对象：\n",
    ">> 1、将本地html文档中的数据加载到该对象中，数据加载与实例化同步实现：\n",
    ">> ```fp=open('test.html','r',encoding='utf-8')```<br>\n",
    ">> ```soup=BeautifulSoup(fp,'lxml')```<br>\n",
    ">> 2、将互联网上提取的页面源码加载至该对象中(使用较多)：<br>\n",
    ">> ```page_text=response.text```<br>\n",
    ">> ```soup=BeautifulSoup(page_text,'lxml')```<br>\n",
    "\n",
    "> bs4中提供的数据解析的方法与属性\n",
    ">> 1、soup.tagName：返回文档中第1次出现的tagNam标签对应的标签内容，如：```soup.div```；<bR>\n",
    ">> 2、```soup.find()```：\n",
    ">>> - ```soup.find('tagName')```：等价于```soup.tagName```。\n",
    ">>> - ```soup.find('tagName',class_/id/attr='attri')```：根据属性定位tagName标签中具体内容；<br>\n",
    "\n",
    ">> 3、```soup.find_all('tagName')```：找到所有的符合标签的内容，返回为列表\n",
    "\n",
    ">> 4、select：<br>\n",
    ">>> ```soup.select('某种选择器(id,class,标签···选择器)')```，返回列表；\n",
    ">>> 层级选择器：<br>  \n",
    ">>> - ```soup.select('.tang > ul > li > a')```: >表示的是一个层级；\n",
    ">>> - ```soup.select('.tang > ul a')```: 空格表示的是多个层级；\n",
    "\n",
    ">> 5、获取标签中的文本数据\n",
    ">>> ```soup.tagName.text/string/get_text()```\n",
    ">>> - ```text/get_text()```：获得标签中所有的文本内容\n",
    ">>> - ```string```：仅可以获得获得标签下直系的文本内容，若没有内容返回None\n",
    "\n",
    ">> 6、获取标签中的属性值\n",
    ">>> - ```soup.tageName['属性标签']```\n",
    "---\n",
    "#### Misssion 5：采用bs4下载书籍章节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://www.shicimingju.com/book/sanguoyanyi.html'\n",
    "    # 为了避免使用text出现乱码，这里采用2进制获取相关内容\n",
    "    page_text = requests.get(url=url,headers=headers).content\n",
    "    soup = BeautifulSoup(page_text,'lxml')\n",
    "    li_list = soup.select('.book-mulu > ul > li')\n",
    "    # print(li_list)\n",
    "    fp = open('sanguo.txt','w',encoding='utf-8')\n",
    "    for li in li_list:\n",
    "        title = li.a.string\n",
    "        detail_url = 'https://www.shicimingju.com/' + li.a['href']\n",
    "        # print(title)\n",
    "        # print(detail_url,'\\n')\n",
    "        chapter = requests.get(url=detail_url,headers=headers).content\n",
    "        detail_soup = BeautifulSoup(chapter,'lxml')\n",
    "        div_tag = detail_soup.find('div',class_='chapter_content')\n",
    "        content = div_tag.text\n",
    "        fp.write(title+':'+content+'\\n\\n')\n",
    "        print('<<'+title+'>>'+'下载完毕！')\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 6：xpath数据解析(常用、便捷、高效、通用性强)，环境：```pip install lxml```\n",
    "##### xpath数据解析原理\n",
    "- 1、实例化一个etree对象，并将需要被解析的页面源码数据加载至该对象中；  \n",
    "```tree = etree.parse('test.html')```  \n",
    "- 2、通过调用etree对象中xpath方法结合xpath表达式实现标签定位和内容捕获；\n",
    "##### etree对象实例化\n",
    "- 1、将本地的html文档中的源码数据加载到etree对象中：  \n",
    "```etree.parse(filePath)```\n",
    "- 2、将互联网上获取的源码数据加载到该对象中：  \n",
    "```etree.HTML('page_text')```\n",
    "##### xpath数据解析主要使用xpath表达式：```xpath('xpath表达式')```\n",
    "- 1、/：表示从根节点开始定位，表示单个层级。```r=etree.xpath('/html/body/div')```返回Element列表\n",
    "- 2、//：表示多个层级。可以表示从任意位置开始定位。```r=etree.xpath('/html//div')```或者```r=etree.xpath('//div')```\n",
    "- 3、属性定位(```tagName[@attrName=\"attrValue\"]```)：<br>\n",
    "```r=tree.xpath('//div[@class=\"song\"]')```\n",
    "- 4、索引定位(索引是从1开始的)：```r=tree.xpath('//div[@class=\"song\"]/p[3]')```\n",
    "- > 5、提取文本内容\n",
    "- >> - ```/text()```：获取标签中直系文本内容```r=tree.xpath('//div[@class=\"tang\"]//li[5]/a/text()')[0]```\n",
    "- >> - ```//text()```：获取标签下所有的文本内容```r=tree.xpath('//div[@class=\"tang\"]//li[5]/a//text()')```\n",
    "- 6、提取属性(tagName/@arrtName)：```r = tree.xpath('//div[@class=\"song\"]/img/@src')```\n",
    "\n",
    "---\n",
    "#### Misssion 7：xpath实战：爬取58二手房中的房源信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "if __name__ == \"__main__\":\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://bj.58.com/ershoufang/'\n",
    "    page_text = requests.get(url=url,headers=headers).text\n",
    "    tree = etree.HTML(page_text)\n",
    "    div_list = tree.xpath('//div[@id=\"__layout\"]//section[@class=\"list\"][1]')\n",
    "    # print(div_list)\n",
    "    fp = open('58.txt','w',encoding='utf-8')\n",
    "    for div in div_list:\n",
    "        title = div.xpath('.//div[@class=\"property-content-title\"]/h3/text()') # 局部解析相对路径\n",
    "        for content in title:\n",
    "            fp.write(content+'\\n')\n",
    "    print('Done！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 8：解析下载4K图片数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "501834c540f8b330c04c1d43ec1666643355d13f80c9caf7e1080236b3fb263f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
