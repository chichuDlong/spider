{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Data Analysis\n",
    "- 正则  \n",
    "- bs4  \n",
    "- xpath\n",
    "---\n",
    "#### 常用正则表达式\n",
    "> 单字符：\n",
    "- . ：处换行外所有字符  \n",
    "- [] ：[aoe] [a-w] 匹配集合中任意一个字符  \n",
    "- \\d ：数字 [0-9]  \n",
    "- \\D ：非数字  \n",
    "- \\w ：数字、字母、下划线、中文  \n",
    "- \\W ：非\\w  \n",
    "- \\s ：所有非空白字符，等价于[\\f\\n\\r\\t\\v]  \n",
    "- \\S ：非空白  \n",
    "> 数量修饰：\n",
    "- \\* ：任意多次，>=0  \n",
    "- \\+ ：至少1次 >=1  \n",
    "- ? ：可有可无，0次或者1次  \n",
    "- {m} ：固定m次 hello{3,}  \n",
    "- {m,} ：至少m次  \n",
    "- {m,n} ：至少m-n次  \n",
    "> 边界：\n",
    "- $ ：以某某结尾\n",
    "- ^ ：以某某开头\n",
    "> 分组：\n",
    "- (ab)\n",
    "- 贪婪模式：.*\n",
    "- 非贪婪模式(惰性)模式：.*?\n",
    "- re.I ：忽略大小写\n",
    "- re.M ：多行匹配\n",
    "- re.S ：单行匹配\n",
    "- re.sub(正则表达式，替换内容，字符串)\n",
    "---\n",
    "#### Mission 1：常用正则练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "hello world\n",
      "['170']\n",
      "['http://', 'https://']\n",
      "hello\n",
      "hit.\n",
      "['saas', 'sas']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 提取出Python\n",
    "key='javapythonc++php'\n",
    "res=re.findall('python',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出hello word\n",
    "key='<html><h1>hello world<h1></html>'\n",
    "res=re.findall('<h1>(.*)<h1>',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出170\n",
    "string='我身高是170'\n",
    "res=re.findall('\\d+',string)\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出http://和https://\n",
    "key='http://www.baidu.com and https://boob.com'\n",
    "res=re.findall('https?://',key)\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出hello\n",
    "key='lalala<html>hello</html>hahah'\n",
    "res=re.findall('<[Hh][Tt][mM][lL]>(.*)</[Hh][Tt][mM][lL]>',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出hit.\n",
    "key='bobo@hit.edu.com'\n",
    "res=re.findall('h.*?\\.',key)[0]\n",
    "print(res)\n",
    "###################################################################\n",
    "# 提取出sas和saas\n",
    "key='saas and sas and saaaas'\n",
    "res=re.findall('sa{1,2}s',key)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 2：爬取图片数据\n",
    "- text——字符串型数据  \n",
    "- content——2进制型数据  \n",
    "- json——对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "if __name__ == \"__main__\":\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://tse1-mm.cn.bing.net/th/id/R-C.47d5fd52caf4fce40d910c06a7282572?rik=OKBN%2btbwWi3Gxw&riu=http%3a%2f%2fwww.sa.buaa.edu.cn%2f__local%2f4%2f7D%2f5F%2fD52CAF4FCE40D910C06A7282572_411891C6_7668.jpg&ehk=O84%2fcaojbXEwILEhWQwc9ZCtqddou1P4SxY3upbS4g0%3d&risl=&pid=ImgRaw&r=0'\n",
    "    img_data = requests.get(url=url,headers=headers).content\n",
    "    with open('hallThruster.jpg','wb') as fp:\n",
    "        fp.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 3：批量获取图片数据——正则方法\n",
    "- step 1：通过爬虫获取整个页面  \n",
    "- step 2：正则解析数据获取需要信息所在地址  \n",
    "- step 3：根据所需信息地址发起请求获取响应数据  \n",
    "- step 4：持久化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片下载完成！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    #UA伪装\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    if not os.path.exists('./picture_libs'):#如果路径不存在，则用mkdir创建一个单级目录\n",
    "        os.mkdir('./picture_libs')        \n",
    "    #这个最大的for循环是用来执行翻页操作的，由于页数不知道，我这边是随便指定结束页尾的\n",
    "    for pageNum in range(1, 3):\n",
    "        # 当你看到输出语句存在 %d， %s， %f等百分号的时候，就接触到python字符串格式化输出相关知识。\n",
    "        url = 'https://desk.zol.com.cn/fengjing/%d.html'#1.指定url\n",
    "        new_url = format(url%pageNum)#学习format函数的用法\n",
    "        #2.发起请求    3.获取响应数据\n",
    "        page_text = requests.get(url=new_url, headers=headers).text\n",
    "\n",
    "        #存储以及打印出page_text中的内容\n",
    "        # with open('./content','w',encoding='utf-8') as fp:\n",
    "        #     fp.write(page_text)\n",
    "        #     fp.write('\\n')#换行\n",
    "\n",
    "        #正则数据解析\n",
    "        ex = 'class=\"photo-list-.*?src=\"(.*?)\"\\stitle\\s=\\s\".*?\".*?<ins>.*?</ins>'\n",
    "        # 根据正则表达式ex提取page_text中的内容，re.S为单行匹配\n",
    "        img_src_list = re.findall(ex, page_text, re.S)\n",
    "        # print(img_src_list)\n",
    "        i = 0\n",
    "        for src in img_src_list:#遍历img_src_list，即1.指定url\n",
    "            #2.发起请求 3.获取响应数据，注意这里的是图片数据，用的是content\n",
    "            img_data = requests.get(url=src, headers=headers).content#对每一个\n",
    "            # print(img_data)\n",
    "            # 将图片地址以字符‘/’分割，最后一个字符串为图片原来地址，为方便区分人为指定图片名称\n",
    "            # img_name=src.split('/')[-1] \n",
    "            i = i + 1\n",
    "            str2 = str(pageNum)+'_'+str(i)#图片的定位是页数+在这页的位置\n",
    "            img_name = str2+'.jpg'#图片名称\n",
    "            img_path = './picture_libs/'+img_name#图片路径\n",
    "            with open(img_path, 'wb', )as fp:#4.持久化存储\n",
    "                fp.write(img_data)\n",
    "                time.sleep(0.5)\n",
    "            # print(img_name,'下载成功！')\n",
    "    print('图片下载完成！')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 4：bs4数据解析(仅限Python)，环境：```pip install bs4```，```pip install lxml```\n",
    "> 数据解析原理：\n",
    ">> 1、标签定位；<br>\n",
    ">> 2、提取标签、标签属性中存储的数据值；\n",
    "\n",
    "> bs4数据解析原理：\n",
    ">> 1、实例化一个BeautifulSoup对象，并且将页面源码数据加载至该对象中；<br>\n",
    ">> 2、通过调用BeautifulSoup对象中相关属性或方法进行标签定位和数据提取；\n",
    "\n",
    "> 如何实例化一个BeautifulSoup对象：\n",
    ">> 1、将本地html文档中的数据加载到该对象中，数据加载与实例化同步实现：\n",
    ">> ```fp=open('test.html','r',encoding='utf-8')```<br>\n",
    ">> ```soup=BeautifulSoup(fp,'lxml')```<br>\n",
    ">> 2、将互联网上提取的页面源码加载至该对象中(使用较多)：<br>\n",
    ">> ```page_text=response.text```<br>\n",
    ">> ```soup=BeautifulSoup(page_text,'lxml')```<br>\n",
    "\n",
    "> bs4中提供的数据解析的方法与属性\n",
    ">> 1、soup.tagName：返回文档中第1次出现的tagNam标签对应的标签内容，如：```soup.div```；<bR>\n",
    ">> 2、```soup.find()```：\n",
    ">>> - ```soup.find('tagName')```：等价于```soup.tagName```。\n",
    ">>> - ```soup.find('tagName',class_/id/attr='attri')```：根据属性定位tagName标签中具体内容；<br>\n",
    "\n",
    ">> 3、```soup.find_all('tagName')```：找到所有的符合标签的内容，返回为列表\n",
    "\n",
    ">> 4、select：<br>\n",
    ">>> ```soup.select('某种选择器(id,class,标签···选择器)')```，返回列表；\n",
    ">>> 层级选择器：<br>  \n",
    ">>> - ```soup.select('.tang > ul > li > a')```: >表示的是一个层级；\n",
    ">>> - ```soup.select('.tang > ul a')```: 空格表示的是多个层级；\n",
    "\n",
    ">> 5、获取标签中的文本数据\n",
    ">>> ```soup.tagName.text/string/get_text()```\n",
    ">>> - ```text/get_text()```：获得标签中所有的文本内容\n",
    ">>> - ```string```：仅可以获得获得标签下直系的文本内容，若没有内容返回None\n",
    "\n",
    ">> 6、获取标签中的属性值\n",
    ">>> - ```soup.tageName['属性标签']```\n",
    "---\n",
    "#### Misssion 5：采用bs4下载书籍章节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://www.shicimingju.com/book/sanguoyanyi.html'\n",
    "    # 为了避免使用text出现乱码，这里采用2进制获取相关内容\n",
    "    page_text = requests.get(url=url,headers=headers).content\n",
    "    soup = BeautifulSoup(page_text,'lxml')\n",
    "    li_list = soup.select('.book-mulu > ul > li')\n",
    "    # print(li_list)\n",
    "    fp = open('sanguo.txt','w',encoding='utf-8')\n",
    "    for li in li_list:\n",
    "        title = li.a.string\n",
    "        detail_url = 'https://www.shicimingju.com/' + li.a['href']\n",
    "        # print(title)\n",
    "        # print(detail_url,'\\n')\n",
    "        chapter = requests.get(url=detail_url,headers=headers).content\n",
    "        detail_soup = BeautifulSoup(chapter,'lxml')\n",
    "        div_tag = detail_soup.find('div',class_='chapter_content')\n",
    "        content = div_tag.text\n",
    "        fp.write(title+':'+content+'\\n\\n')\n",
    "        print('<<'+title+'>>'+'下载完毕！')\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 6：xpath数据解析(常用、便捷、高效、通用性强)，环境：```pip install lxml```\n",
    "##### xpath数据解析原理\n",
    "- 1、实例化一个etree对象，并将需要被解析的页面源码数据加载至该对象中；  \n",
    "```tree = etree.parse('test.html')```  \n",
    "- 2、通过调用etree对象中xpath方法结合xpath表达式实现标签定位和内容捕获；\n",
    "##### etree对象实例化\n",
    "- 1、将本地的html文档中的源码数据加载到etree对象中：  \n",
    "```etree.parse(filePath)```\n",
    "- 2、将互联网上获取的源码数据加载到该对象中：  \n",
    "```etree.HTML('page_text')```\n",
    "##### xpath数据解析主要使用xpath表达式：```xpath('xpath表达式')```\n",
    "- 1、/：表示从根节点开始定位，表示单个层级。```r=etree.xpath('/html/body/div')```返回Element列表\n",
    "- 2、//：表示多个层级。可以表示从任意位置开始定位。```r=etree.xpath('/html//div')```或者```r=etree.xpath('//div')```\n",
    "- 3、属性定位(```tagName[@attrName=\"attrValue\"]```)：<br>\n",
    "```r=tree.xpath('//div[@class=\"song\"]')```\n",
    "- 4、索引定位(索引是从1开始的)：```r=tree.xpath('//div[@class=\"song\"]/p[3]')```\n",
    "- > 5、提取文本内容\n",
    "- >> - ```/text()```：获取标签中直系文本内容```r=tree.xpath('//div[@class=\"tang\"]//li[5]/a/text()')[0]```\n",
    "- >> - ```//text()```：获取标签下所有的文本内容```r=tree.xpath('//div[@class=\"tang\"]//li[5]/a//text()')```\n",
    "- 6、提取属性(tagName/@arrtName)：```r = tree.xpath('//div[@class=\"song\"]/img/@src')```\n",
    "\n",
    "---\n",
    "#### Misssion 7：xpath实战：爬取58二手房中的房源信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "if __name__ == \"__main__\":\n",
    "    headers={\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://bj.58.com/ershoufang/'\n",
    "    page_text = requests.get(url=url,headers=headers).text\n",
    "    tree = etree.HTML(page_text)\n",
    "    div_list = tree.xpath('//div[@id=\"__layout\"]//section[@class=\"list\"][1]')\n",
    "    # print(div_list)\n",
    "    fp = open('58.txt','w',encoding='utf-8')\n",
    "    for div in div_list:\n",
    "        title = div.xpath('.//div[@class=\"property-content-title\"]/h3/text()') # 局部解析相对路径\n",
    "        for content in title:\n",
    "            fp.write(content+'\\n')\n",
    "    print('Done！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 8：解析下载4K图片数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兰博基尼Huracan GT3 EVO2 个性绿色跑车5k壁纸.jpg 下载完成！\n",
      "兰博基尼Huracan4k电脑壁纸3840x2160.jpg 下载完成！\n",
      "rolls royce phantom orchid劳斯莱斯幻影兰花4k汽车壁纸.jpg 下载完成！\n",
      "noble m500 蓝色 4k跑车壁纸.jpg 下载完成！\n",
      "noble_m500 8k跑车壁纸7680x4320.jpg 下载完成！\n",
      "法拉利ferrari sf90 spider黄色跑车4k壁纸.jpg 下载完成！\n",
      "华丽警车 techart cabriolet tune it safe concept 4k壁纸.jpg 下载完成！\n",
      "法拉利488 GTB跑车4k电脑壁纸3840x2160.jpg 下载完成！\n",
      "兰博基尼countach lp500概念跑车8k壁纸7680x4320.jpg 下载完成！\n",
      "迈凯伦600LT跑车4k电脑壁纸.jpg 下载完成！\n",
      "布加迪 波利德(Bugatti Bolide) 5k跑车壁纸5120x2880.jpg 下载完成！\n",
      "兰博基尼 Countach Lpi 800传奇超级跑车4k高清壁纸.jpg 下载完成！\n",
      "劳斯莱斯 梦幻概念车3440x1440带鱼屏壁纸.jpg 下载完成！\n",
      "劳斯莱斯 梦幻概念车4k壁纸.jpg 下载完成！\n",
      "迈凯伦2021 McLaren MCL35M 超级跑车4k壁纸.jpg 下载完成！\n",
      "兰博基尼terzo millennio概念车3440x1440带鱼屏壁纸.jpg 下载完成！\n",
      "兰博基尼terzo millennio概念车4k超级跑车壁纸.jpg 下载完成！\n",
      "法拉利v12 versione speciale 5k汽车壁纸.jpg 下载完成！\n",
      "法拉利sf90 spider assetto fiorano 5k壁纸.jpg 下载完成！\n",
      "兰博基尼huracan sto lounge nyc 2021 5k跑车壁纸.jpg 下载完成！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import os\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # URL地址\n",
    "    url = 'http://pic.netbian.com/4kqiche/'\n",
    "    headers = {\n",
    "        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.116 Safari/537.36'\n",
    "    }\n",
    "    # 创建文件夹保存图片数据\n",
    "    if not os.path.exists('./4kPic'):\n",
    "        os.mkdir('./4kPic')\n",
    "\n",
    "    # 爬取页面源数据\n",
    "    response = requests.get(url=url,headers=headers)\n",
    "    # response.encoding = 'gbk'# utf-8\n",
    "    page_text = response.text\n",
    "\n",
    "    # 数据解析：src属性值、alt属性值\n",
    "    tree = etree.HTML(page_text)\n",
    "    li_list = tree.xpath('//div[@class=\"slist\"]/ul/li')\n",
    "    for li in li_list:\n",
    "        # 获取图片的src属性值\n",
    "        img_src = 'http://pic.netbian.com/'+ li.xpath('./a/img/@src')[0]\n",
    "        # 获取图片的alt属性值\n",
    "        img_name = li.xpath('./a/img/@alt')[0]+'.jpg'\n",
    "        img_name = img_name.encode('iso-8859-1').decode('gbk')\n",
    "        # print(img_name,img_src)\n",
    "        # 访问图片地址\n",
    "        img_data = requests.get(url=img_src,headers=headers).content\n",
    "        # 持久化存储图片数据\n",
    "        img_path = './4kPic/'+img_name\n",
    "        with open(img_path,'wb') as fp:\n",
    "            fp.write(img_data)\n",
    "            print(img_name,'下载完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 9：获取全国城市名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '上海', '广州', '深圳', '杭州', '天津', '成都', '南京', '西安', '武汉', '阿坝州', '安康', '阿克苏地区', '阿里地区', '阿拉善盟', '阿勒泰地区', '安庆', '安顺', '鞍山', '克孜勒苏州', '安阳', '蚌埠', '白城', '保定', '北海', '宝鸡', '北京', '毕节', '博州', '白山', '百色', '保山', '白沙', '包头', '保亭', '本溪', '巴彦淖尔', '白银', '巴中', '滨州', '亳州', '长春', '昌都', '常德', '成都', '承德', '赤峰', '昌吉州', '五家渠', '昌江', '澄迈', '重庆', '长沙', '常熟', '楚雄州', '朝阳', '沧州', '长治', '常州', '潮州', '郴州', '池州', '崇左', '滁州', '定安', '丹东', '东方', '东莞', '德宏州', '大理州', '大连', '大庆', '大同', '定西', '大兴安岭地区', '德阳', '东营', '黔南州', '达州', '德州', '儋州', '鄂尔多斯', '恩施州', '鄂州', '防城港', '佛山', '抚顺', '阜新', '阜阳', '富阳', '抚州', '福州', '广安', '贵港', '桂林', '果洛州', '甘南州', '固原', '广元', '贵阳', '甘孜州', '赣州', '广州', '淮安', '海北州', '鹤壁', '淮北', '河池', '海东地区', '邯郸', '哈尔滨', '合肥', '鹤岗', '黄冈', '黑河', '红河州', '怀化', '呼和浩特', '海口', '呼伦贝尔', '葫芦岛', '哈密地区', '海门', '海南州', '淮南', '黄南州', '衡水', '黄山', '黄石', '和田地区', '海西州', '河源', '衡阳', '汉中', '杭州', '菏泽', '贺州', '湖州', '惠州', '吉安', '金昌', '晋城', '景德镇', '金华', '西双版纳州', '九江', '吉林', '即墨', '江门', '荆门', '佳木斯', '济南', '济宁', '胶南', '酒泉', '句容', '湘西州', '金坛', '鸡西', '嘉兴', '江阴', '揭阳', '济源', '嘉峪关', '胶州', '焦作', '锦州', '晋中', '荆州', '库尔勒', '开封', '黔东南州', '克拉玛依', '昆明', '喀什地区', '昆山', '临安', '六安', '来宾', '聊城', '临沧', '娄底', '乐东', '廊坊', '临汾', '临高', '漯河', '丽江', '吕梁', '陇南', '六盘水', '拉萨', '乐山', '丽水', '凉山州', '陵水', '莱芜', '莱西', '临夏州', '溧阳', '辽阳', '辽源', '临沂', '龙岩', '洛阳', '连云港', '莱州', '兰州', '林芝', '柳州', '泸州', '马鞍山', '牡丹江', '茂名', '眉山', '绵阳', '梅州', '宁波', '南昌', '南充', '宁德', '内江', '南京', '怒江州', '南宁', '南平', '那曲地区', '南通', '南阳', '平度', '平顶山', '普洱', '盘锦', '蓬莱', '平凉', '莆田', '萍乡', '濮阳', '攀枝花', '青岛', '琼海', '秦皇岛', '曲靖', '潜江', '齐齐哈尔', '七台河', '黔西南州', '清远', '庆阳', '钦州', '衢州', '泉州', '琼中', '荣成', '日喀则', '乳山', '日照', '韶关', '寿光', '上海', '绥化', '石河子', '石家庄', '商洛', '三明', '三门峡', '山南', '遂宁', '神农架林区', '四平', '商丘', '宿迁', '上饶', '汕头', '汕尾', '绍兴', '三亚', '邵阳', '沈阳', '十堰', '松原', '双鸭山', '深圳', '朔州', '宿州', '随州', '苏州', '石嘴山', '泰安', '塔城地区', '太仓', '铜川', '屯昌', '通化', '天津', '铁岭', '通辽', '铜陵', '吐鲁番地区', '天门', '铜仁地区', '唐山', '天水', '太原', '台州', '泰州', '文昌', '文登', '潍坊', '瓦房店', '威海', '乌海', '芜湖', '武汉', '吴江', '乌兰察布', '乌鲁木齐', '渭南', '万宁', '文山州', '武威', '无锡', '温州', '吴忠', '梧州', '五指山', '西安', '兴安盟', '许昌', '宣城', '襄阳', '孝感', '迪庆州', '辛集', '锡林郭勒盟', '厦门', '西宁', '咸宁', '湘潭', '邢台', '仙桃', '新乡', '咸阳', '新余', '信阳', '忻州', '徐州', '雅安', '延安', '延边州', '宜宾', '盐城', '宜昌', '宜春', '银川', '运城', '伊春', '云浮', '阳江', '营口', '榆林', '玉林', '伊犁哈萨克州', '阳泉', '玉树州', '烟台', '鹰潭', '义乌', '宜兴', '玉溪', '益阳', '岳阳', '扬州', '永州', '淄博', '自贡', '珠海', '湛江', '镇江', '诸暨', '张家港', '张家界', '张家口', '周口', '驻马店', '章丘', '肇庆', '中山', '舟山', '昭通', '中卫', '张掖', '招远', '资阳', '遵义', '枣庄', '漳州', '郑州', '株洲']\n",
      "399\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.116 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://www.aqistudy.cn/historydata/'\n",
    "    response = requests.get(url = url, headers = headers)\n",
    "    page_text = response.text\n",
    "    tree = etree.HTML(page_text)\n",
    "    #方法一：\n",
    "    # # 热门城市\n",
    "    # host_city_list = tree.xpath('//div[@class=\"bottom\"]/ul/li')\n",
    "    # host_name_list = []\n",
    "    # for li in  host_city_list:\n",
    "    #     host_name = li.xpath('./a/text()')[0]\n",
    "    #     host_name_list.append(host_name)\n",
    "    # # print(host_name_list)\n",
    "    #\n",
    "    # #1.\n",
    "    # # all_city_list = []\n",
    "    # # all_city_ul_list = tree.xpath('//div[@class=\"bottom\"]/ul')\n",
    "    # # for ul in all_city_ul_list:\n",
    "    # #     get_li_list = ul.xpath('./div/li')\n",
    "    # #     for li in get_li_list:\n",
    "    # #         name = li.xpath('./a/text()')[0]\n",
    "    # #         host_name_list.append(name)\n",
    "    # #2.\n",
    "    # # all_city_li = tree.xpath('//div[@class=\"bottom\"]/ul/div[2]/li')\n",
    "    # # for li in all_city_li:\n",
    "    # #     name = li.xpath('./a/text()')[0]\n",
    "    # #     host_name_list.append(name)\n",
    "    # print(host_name_list)\n",
    "    # print(len(host_name_list))\n",
    "\n",
    "    #方法二：\n",
    "    a_list = tree.xpath('//div[@class=\"bottom\"]/ul/li/a | //div[@class=\"bottom\"]/ul/div[2]/li/a')\n",
    "    all_city_names = []\n",
    "    for a in a_list:\n",
    "        city_name = a.xpath('./text()')[0]\n",
    "        all_city_names.append(city_name)\n",
    "    print(all_city_names)\n",
    "    print(len(all_city_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Mission 10：提取简历模板(https://sc.chinaz.com/jianli/free.html)\n",
    "- 仅参考，对应的xpath地址实时变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "from multiprocessing.dummy import Pool\n",
    "import os\n",
    "\n",
    "#获取单个页面的网页资源\n",
    "def getMainPage(url):\n",
    "    headers = {\n",
    "        'User_Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers)\n",
    "    return response\n",
    "\n",
    "#爬取所有模板\n",
    "def downLoadModel(pageText):\n",
    "    tree = etree.HTML(pageText)\n",
    "    #获取当前页面所有模板链接\n",
    "    srcList = tree.xpath('//div[@class=\"box col3 ws_block\"]/a/@href')\n",
    "    if not os.path.exists('resources/简历模板'):\n",
    "        os.mkdir('resources/简历模板')\n",
    "    #由于访问量比较大，使用进程池提速\n",
    "    pool = Pool(len(srcList))\n",
    "    pool.map(downLoad, srcList)\n",
    "\n",
    "#下载单个模板\n",
    "def downLoad(i):\n",
    "    src = 'https:' + i\n",
    "    iPage = getMainPage(src).content\n",
    "    tree = etree.HTML(iPage)\n",
    "    #获取下载链接\n",
    "    downLoadUrl = tree.xpath('//div[@class=\"clearfix mt20 downlist\"]/ul/li/a/@href')[0]\n",
    "    #获取模板名字\n",
    "    dataName = tree.xpath('//div[@class=\"ppt_left fl\"]/div/div/h1/text()')[0]\n",
    "    data = getMainPage(downLoadUrl).content\n",
    "    with open('resources/简历模板/' + dataName + '.rar', 'wb') as fp:\n",
    "        fp.write(data)\n",
    "    print(dataName + ' 保存成功！')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'https://sc.chinaz.com/jianli/free.html'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "501834c540f8b330c04c1d43ec1666643355d13f80c9caf7e1080236b3fb263f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
